{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f75550-19bc-456a-859c-ba7298fe30c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "target_dir = \"./\"\n",
    "\n",
    "# === Paths ===\n",
    "DATA_PATH = Path(target_dir + \"bioasq.json\")      # <-- set to your actual BioASQ JSON filename\n",
    "DB_DIR    = Path(target_dir + \"chroma_bioasq\")    # persistent directory for Chroma\n",
    "DB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === Collection ===\n",
    "COLLECTION_NAME = \"bioasq_q2q\"\n",
    "\n",
    "# === Embedding model ===\n",
    "# Choose ONE of these:\n",
    "#   \"BAAI/bge-small-en-v1.5\"  (fast, 384-d)  -- default\n",
    "#   \"intfloat/e5-base-v2\"     (requires 'query:'/'passage:' prefixes)\n",
    "EMBED_MODEL_NAME = os.environ.get(\"EMBED_MODEL_NAME\", \"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# === Index behavior ===\n",
    "REBUILD_INDEX = False   # set True to drop & rebuild the collection\n",
    "\n",
    "# === Optional LLM refinement ===\n",
    "# Option A: local Ollama (install app, run `ollama pull mistral` or `ollama pull llama3:8b`)\n",
    "USE_OLLAMA     = True\n",
    "OLLAMA_MODEL   = os.environ.get(\"OLLAMA_MODEL\", \"gemma3:1b\")  # e.g. \"mistral\", \"llama3:8b\"\n",
    "\n",
    "# Option B: OpenRouter (free-tier friendly). Create an API key and export:\n",
    "#   export OPENROUTER_API_KEY=\"...\"\n",
    "USE_OPENROUTER = False\n",
    "OPENROUTER_MODEL = os.environ.get(\"OPENROUTER_MODEL\", \"mistralai/mistral-7b-instruct:free\")\n",
    "\n",
    "# If both are True, we'll try Ollama first, then OpenRouter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba19193b-8de0-48cf-999d-69c87bfb1fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def load_bioasq(path: Path) -> List[Dict]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    # expecting {\"questions\": [ {...}, ... ]}\n",
    "    return data[\"questions\"]\n",
    "\n",
    "def to_qas(records: List[Dict]) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Return list of (id, question, answer) tuples.\n",
    "    Favors 'ideal_answer' (joined) else tries 'exact_answer'.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i, r in enumerate(records):\n",
    "        qid = r.get(\"id\") or f\"bioasq-{i}\"\n",
    "        question = (r.get(\"body\") or \"\").strip()\n",
    "        ans = \"\"\n",
    "        if r.get(\"ideal_answer\"):\n",
    "            # 'ideal_answer' is often a list of strings; join with space\n",
    "            if isinstance(r[\"ideal_answer\"], list):\n",
    "                ans = \" \".join(s.strip() for s in r[\"ideal_answer\"] if s and isinstance(s, str)).strip()\n",
    "            elif isinstance(r[\"ideal_answer\"], str):\n",
    "                ans = r[\"ideal_answer\"].strip()\n",
    "        if not ans and r.get(\"exact_answer\"):\n",
    "            if isinstance(r[\"exact_answer\"], list):\n",
    "                # Could be list of lists; flatten\n",
    "                flat = []\n",
    "                for item in r[\"exact_answer\"]:\n",
    "                    if isinstance(item, list):\n",
    "                        flat.extend(item)\n",
    "                    elif isinstance(item, str):\n",
    "                        flat.append(item)\n",
    "                ans = \"; \".join(s.strip() for s in flat if s and isinstance(s, str)).strip()\n",
    "            elif isinstance(r[\"exact_answer\"], str):\n",
    "                ans = r[\"exact_answer\"].strip()\n",
    "        out.append((qid, question, ans))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e5b1517-1943-4c93-9ffa-da6fa49d20c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Huzaifa\\anaconda3\\envs\\bioasq-rag\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from typing import Sequence\n",
    "\n",
    "class Embedder:\n",
    "    \"\"\"\n",
    "    Handles model loading and the query/document prefix rules.\n",
    "    - E5 models: 'query: ' / 'passage: '  (required)  [Microsoft/Zilliz & SBERT docs]\n",
    "    - BGE models: recommended query instruction prompt (docs often recommend adding it)  [SBERT/Haystack]\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, device: str | None = None):\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "        self.model.max_seq_length = 512  # safe default\n",
    "\n",
    "        name = model_name.lower()\n",
    "        self.is_e5  = \"e5\"  in name\n",
    "        self.is_bge = \"bge\" in name\n",
    "\n",
    "        # Recommended query instruction for BGE (English)\n",
    "        self.bge_query_instruction = \"Represent this sentence for searching relevant passages: \"\n",
    "\n",
    "    def _prep_docs(self, texts: Sequence[str]) -> list[str]:\n",
    "        if self.is_e5:\n",
    "            return [f\"passage: {t}\" for t in texts]\n",
    "        # BGE doesn’t require a doc prefix; empirically it’s fine as-is.\n",
    "        return list(texts)\n",
    "\n",
    "    def _prep_query(self, text: str) -> str:\n",
    "        if self.is_e5:\n",
    "            return f\"query: {text}\"\n",
    "        if self.is_bge:\n",
    "            return f\"{self.bge_query_instruction}{text}\"\n",
    "        return text\n",
    "\n",
    "    def embed_docs(self, texts: Sequence[str], batch_size: int = 64) -> np.ndarray:\n",
    "        prepped = self._prep_docs(texts)\n",
    "        return self.model.encode(\n",
    "            prepped, batch_size=batch_size, normalize_embeddings=True, convert_to_numpy=True, show_progress_bar=True\n",
    "        )\n",
    "\n",
    "    def embed_query(self, text: str) -> np.ndarray:\n",
    "        prepped = self._prep_query(text)\n",
    "        return self.model.encode([prepped], normalize_embeddings=True, convert_to_numpy=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d7bdb13-eb83-46b5-92b5-dd413bf06534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "C:\\Users\\Huzaifa\\anaconda3\\envs\\bioasq-rag\\lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5049 Q/A pairs from bioasq.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|█████████████████████████████████████████████████████████████████████████████| 79/79 [02:05<00:00,  1.59s/it]\n",
      "Adding to Chroma:   0%|                                                                             | 0/20 [00:00<?, ?it/s]Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n",
      "Adding to Chroma: 100%|████████████████████████████████████████████████████████████████████| 20/20 [00:11<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chroma] Built collection 'bioasq_q2q' with 5049 items.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions  # (not used, but handy for alternatives)\n",
    "from tqdm import tqdm\n",
    "import uuid\n",
    "from typing import Optional\n",
    "from chromadb.api.models import Collection \n",
    "\n",
    "def get_client(path: Path):\n",
    "    return chromadb.PersistentClient(path=str(path))  # persists to disk\n",
    "\n",
    "def get_or_create_collection(client, name: str):\n",
    "    # Set cosine distance for nearest-neighbor search\n",
    "    # (HNSW space can be controlled via metadata/config; cosine is convenient for normalized embeddings)\n",
    "    try:\n",
    "        return client.get_collection(name=name)\n",
    "    except Exception:\n",
    "        return client.create_collection(name=name, metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "def drop_collection_if_exists(client, name: str):\n",
    "    try:\n",
    "        client.delete_collection(name=name)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def build_index(\n",
    "    data_path: Path,\n",
    "    db_dir: Path,\n",
    "    collection_name: str = COLLECTION_NAME,\n",
    "    model_name: str = EMBED_MODEL_NAME,\n",
    "    rebuild: bool = REBUILD_INDEX,\n",
    "    batch_size: int = 256,\n",
    ") -> tuple[Collection, Embedder]:\n",
    "    client = get_client(db_dir)\n",
    "\n",
    "    if rebuild:\n",
    "        drop_collection_if_exists(client, collection_name)\n",
    "\n",
    "    collection = get_or_create_collection(client, collection_name)\n",
    "    embedder = Embedder(model_name)\n",
    "\n",
    "    # If already populated and not rebuilding, skip\n",
    "    try:\n",
    "        existing_count = collection.count()\n",
    "    except Exception:\n",
    "        existing_count = 0\n",
    "\n",
    "    if existing_count and not rebuild:\n",
    "        print(f\"[Chroma] Reusing collection '{collection_name}' with {existing_count} items.\")\n",
    "        return collection, embedder\n",
    "\n",
    "    # Load data\n",
    "    records = load_bioasq(data_path)\n",
    "    qas = [(qid, q, a) for (qid, q, a) in to_qas(records) if q and a]\n",
    "    print(f\"Loaded {len(qas)} Q/A pairs from {data_path.name}\")\n",
    "\n",
    "    # Prepare batches\n",
    "    ids, docs, metas = [], [], []\n",
    "    questions = [q for _, q, _ in qas]\n",
    "    answers   = [a for _, _, a in qas]\n",
    "    qids      = [qid for qid, _, _ in qas]\n",
    "\n",
    "    # Compute embeddings for questions\n",
    "    all_embs = embedder.embed_docs(questions, batch_size=64)\n",
    "\n",
    "    # Add to collection in chunks\n",
    "    for i in tqdm(range(0, len(qas), batch_size), desc=\"Adding to Chroma\"):\n",
    "        sl = slice(i, i + batch_size)\n",
    "        chunk_ids   = [qids[j] or str(uuid.uuid4()) for j in range(*sl.indices(len(qas)))]\n",
    "        chunk_docs  = [questions[j] for j in range(*sl.indices(len(qas)))]\n",
    "        chunk_metas = [{\"answer\": answers[j]} for j in range(*sl.indices(len(qas)))]\n",
    "\n",
    "        collection.add(\n",
    "            ids=chunk_ids,\n",
    "            documents=chunk_docs,\n",
    "            metadatas=chunk_metas,\n",
    "            embeddings=all_embs[sl].tolist(),  # pass precomputed embeddings\n",
    "        )\n",
    "\n",
    "    print(f\"[Chroma] Built collection '{collection_name}' with {collection.count()} items.\")\n",
    "    return collection, embedder\n",
    "\n",
    "collection, embedder = build_index(DATA_PATH, DB_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95f00163-68c2-4c1f-becf-9ec6b07f02ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def retrieve_answer(\n",
    "    user_question: str,\n",
    "    k: int = 5,\n",
    "    return_matches: bool = True,\n",
    ") -> dict[str, Any]:\n",
    "    q_emb = embedder.embed_query(user_question)  # 1 x d\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[q_emb.tolist()],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\", \"embeddings\"],\n",
    "    )\n",
    "    # results are columnar; unwrap first query\n",
    "    ids = results.get(\"ids\", [[]])[0]\n",
    "    docs = results.get(\"documents\", [[]])[0]\n",
    "    metas = results.get(\"metadatas\", [[]])[0]\n",
    "    dists = results.get(\"distances\", [[]])[0]\n",
    "\n",
    "    if not ids:\n",
    "        return {\"answer\": None, \"matches\": []}\n",
    "\n",
    "    best = {\n",
    "        \"id\": ids[0],\n",
    "        \"matched_question\": docs[0],\n",
    "        \"retrieved_answer\": metas[0].get(\"answer\", \"\"),\n",
    "        \"distance\": dists[0],\n",
    "    }\n",
    "\n",
    "    payload = {\"answer\": best[\"retrieved_answer\"], \"match\": best}\n",
    "    if return_matches:\n",
    "        payload[\"matches\"] = [\n",
    "            {\n",
    "                \"id\": ids[i],\n",
    "                \"matched_question\": docs[i],\n",
    "                \"retrieved_answer\": metas[i].get(\"answer\", \"\"),\n",
    "                \"distance\": dists[i],\n",
    "            }\n",
    "            for i in range(len(ids))\n",
    "        ]\n",
    "    return payload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35607d89-ce3f-4495-8287-65feaa5db55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_with_ollama(question: str, raw_answer: str, matched_question: str, model: str = OLLAMA_MODEL) -> str | None:\n",
    "    if not USE_OLLAMA:\n",
    "        return None\n",
    "    try:\n",
    "        import ollama\n",
    "        prompt = (\n",
    "            \"You are a biomedical assistant. Using the retrieved answer, produce a concise, direct answer.\\n\"\n",
    "            \"If the retrieved answer already looks complete, lightly paraphrase for clarity without adding facts.\\n\\n\"\n",
    "            f\"User question: {question}\\n\"\n",
    "            f\"Retrieved (matched) question: {matched_question}\\n\"\n",
    "            f\"Retrieved answer: {raw_answer}\\n\\n\"\n",
    "            \"Final answer (one short paragraph):\"\n",
    "        )\n",
    "        resp = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        return resp[\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[Ollama] Skipping refinement ({e})\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c82df99-8cec-49dc-a6a3-c3419fdc4342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(user_question: str, k: int = 5, refine: bool = True) -> dict:\n",
    "    ret = retrieve_answer(user_question, k=k, return_matches=True)\n",
    "    raw = ret[\"answer\"]\n",
    "    match_q = ret[\"match\"][\"matched_question\"] if ret.get(\"match\") else \"\"\n",
    "\n",
    "    final_answer = raw\n",
    "    if refine and raw:\n",
    "        refined = refine_with_ollama(user_question, raw, match_q)\n",
    "        if refined:\n",
    "            final_answer = refined\n",
    "\n",
    "    return {\n",
    "        \"question\": user_question,\n",
    "        \"final_answer\": final_answer,\n",
    "        \"retrieved_answer\": raw,\n",
    "        \"matched_question\": match_q,\n",
    "        \"matches\": ret.get(\"matches\", []),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a903036-030e-44f4-9077-3db5b2e48cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Is Hirschsprung disease Mendelian or multifactorial?\n",
      "\n",
      "Matched question: Is Hirschsprung disease a mendelian or a multifactorial disorder?\n",
      "\n",
      "Answer (final): Hirschsprung disease is considered a complex disorder, with a mix of genetic and environmental factors potentially contributing. While Mendelian forms are prevalent, sporadic cases are often linked to multiplicative inheritance, involving multiple loci.\n",
      "\n",
      "Answer (raw retrieved): Coding sequence mutations in RET, GDNF, EDNRB, EDN3, and SOX10 are involved in the development of Hirschsprung disease. The majority of these genes was shown to be related to Mendelian syndromic forms of Hirschsprung's disease, whereas the non-Mendelian inheritance of sporadic non-syndromic Hirschsprung disease proved to be complex; involvement of multiple loci was demonstrated in a multiplicative model.\n",
      "\n",
      "Top matches (id, distance):\n",
      " - 55031181e9bde69634000014 (distance=0.0341)\n",
      " - 5503121de9bde69634000019 (distance=0.2310)\n",
      " - 55391825bc4f83e828000016 (distance=0.2424)\n",
      " - 551910c5622b194345000007 (distance=0.2450)\n",
      " - 55001420e9bde69634000005 (distance=0.2524)\n"
     ]
    }
   ],
   "source": [
    "# Try with a sample query (adjust to your dataset).\n",
    "sample_q = \"Is Hirschsprung disease Mendelian or multifactorial?\"\n",
    "result = answer_question(sample_q, k=5, refine=True)\n",
    "\n",
    "print(\"Q:\", result[\"question\"])\n",
    "print(\"\\nMatched question:\", result[\"matched_question\"])\n",
    "print(\"\\nAnswer (final):\", result[\"final_answer\"])\n",
    "print(\"\\nAnswer (raw retrieved):\", result[\"retrieved_answer\"])\n",
    "print(\"\\nTop matches (id, distance):\")\n",
    "for m in result[\"matches\"][:5]:\n",
    "    print(\" -\", m[\"id\"], f\"(distance={m['distance']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d046cd-7ef7-4710-990a-18fe4a109d34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
